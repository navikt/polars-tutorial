{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe2acc51",
   "metadata": {},
   "source": [
    "# Polars - Blazingly fast DataFrames in 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed07205",
   "metadata": {},
   "source": [
    "It has been over 2 years already since [I created my first notebook about Polars](https://github.com/r-brink/polars-tutorial/blob/master/polars-tutorial.ipynb) ([Medium post here](https://r-brink.medium.com/introduction-to-polars-ee9e638dc163)). Back then a small project started by Ritchie Vink with promising performance. 2 years later, the project has over 10.000 stars on [Github](https://github.com/pola-rs/polars) and is still leading many performance benchmarks. \n",
    "\n",
    "In this updated tutorial we will revisit the original analyses and rewrite the queries following the latest version of Polars and the Polars book.\n",
    "\n",
    "The structure of this repo:\n",
    "\n",
    "\n",
    "> Remarks written in the earlier notebook are marked like this (as a quote)\n",
    "\n",
    "\n",
    "```python\n",
    "import polars as pl\n",
    "\n",
    "# code snippets from the previous notebook are commented out in the same cell\n",
    "# so you can compare them easily\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d4283c",
   "metadata": {},
   "source": [
    "## Installing Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17feb66b",
   "metadata": {},
   "source": [
    "The first notebook ran with Polars `0.7.0`. Definitely time for an update.\n",
    "\n",
    "First install Polars in your virtual environment. In this notebook we will work with Polars version 0.15.7. Add this to the pip install to ensure all cells run.\n",
    "\n",
    "```shell\n",
    "pip install polars==0.15.7\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d602caf8",
   "metadata": {},
   "source": [
    "### Importing relevant packages and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b047b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1332309",
   "metadata": {},
   "source": [
    "Nothing changed here. Still `import polars as pl`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9851fb40",
   "metadata": {},
   "source": [
    "> Polars already offers many functionalities that we are already familiar if you have worked with Pandas before. We can find an overview, including examples (for most), in the reference guide.\n",
    "\n",
    "Find the dataset that we are going to use here: [Winemag-dataset](https://medium.com/p/48db12c0f067/edit#%20find%20the%20dataset%20here:%20https://www.kaggle.com/zynicide/wine-reviews/?select=winemag-data_first150k.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d37aa19f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file or directory: winemag-data_first150k.csv",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mwinemag-data_first150k.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/polars-tutorial/venv/lib/python3.10/site-packages/polars/io/csv/functions.py:355\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(source, has_header, columns, new_columns, separator, comment_char, quote_char, skip_rows, dtypes, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_count_name, row_count_offset, sample_size, eol_char)\u001B[0m\n\u001B[1;32m    347\u001B[0m         dtypes \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    348\u001B[0m             new_to_current\u001B[38;5;241m.\u001B[39mget(column_name, column_name): column_dtype\n\u001B[1;32m    349\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m column_name, column_dtype \u001B[38;5;129;01min\u001B[39;00m dtypes\u001B[38;5;241m.\u001B[39mitems()\n\u001B[1;32m    350\u001B[0m         }\n\u001B[1;32m    352\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _prepare_file_arg(\n\u001B[1;32m    353\u001B[0m     source, encoding\u001B[38;5;241m=\u001B[39mencoding, use_pyarrow\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mstorage_options\n\u001B[1;32m    354\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m data:\n\u001B[0;32m--> 355\u001B[0m     df \u001B[38;5;241m=\u001B[39m \u001B[43mpl\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_read_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhas_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhas_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mprojection\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m        \u001B[49m\u001B[43mseparator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mseparator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomment_char\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomment_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquote_char\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquote_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskip_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtypes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtypes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnull_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnull_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmissing_utf8_is_empty_string\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmissing_utf8_is_empty_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtry_parse_dates\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtry_parse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[43m        \u001B[49m\u001B[43minfer_schema_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minfer_schema_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_rows\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf8-lossy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    373\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlow_memory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlow_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrechunk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrechunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskip_rows_after_header\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mskip_rows_after_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrow_count_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrow_count_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrow_count_offset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrow_count_offset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[43meol_char\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meol_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_columns:\n\u001B[1;32m    383\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _update_columns(df, new_columns)\n",
      "File \u001B[0;32m~/PycharmProjects/polars-tutorial/venv/lib/python3.10/site-packages/polars/dataframe/frame.py:780\u001B[0m, in \u001B[0;36mDataFrame._read_csv\u001B[0;34m(cls, source, has_header, columns, separator, comment_char, quote_char, skip_rows, dtypes, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_count_name, row_count_offset, sample_size, eol_char)\u001B[0m\n\u001B[1;32m    773\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    774\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcannot use glob patterns and integer based projection as `columns`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    775\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m argument; Use columns: List[str]\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    776\u001B[0m         )\n\u001B[1;32m    778\u001B[0m projection, columns \u001B[38;5;241m=\u001B[39m handle_projection_columns(columns)\n\u001B[0;32m--> 780\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df \u001B[38;5;241m=\u001B[39m \u001B[43mPyDataFrame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    781\u001B[0m \u001B[43m    \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    782\u001B[0m \u001B[43m    \u001B[49m\u001B[43minfer_schema_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    783\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    784\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhas_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    785\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_errors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    786\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    787\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_rows\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    788\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprojection\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    789\u001B[0m \u001B[43m    \u001B[49m\u001B[43mseparator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrechunk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_threads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype_slice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlow_memory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcomment_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquote_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprocessed_null_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmissing_utf8_is_empty_string\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtry_parse_dates\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    803\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskip_rows_after_header\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_prepare_row_count_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrow_count_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrow_count_offset\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    805\u001B[0m \u001B[43m    \u001B[49m\u001B[43msample_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    806\u001B[0m \u001B[43m    \u001B[49m\u001B[43meol_char\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meol_char\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    807\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: No such file or directory: winemag-data_first150k.csv"
     ]
    }
   ],
   "source": [
    "df = pl.read_csv('winemag-data_first150k.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3db072",
   "metadata": {},
   "source": [
    "### Basic dataframe inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c1155",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a30b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53f9812",
   "metadata": {},
   "source": [
    "Nothing has changed so far, until now. Instead of the Pandas-like syntax we used two years ago. We will use the expressions, as recommended by Polars.\n",
    "\n",
    "> We strongly recommend selecting data with expressions for almost all use cases. Square bracket indexing is perhaps useful when doing exploratory data analysis in a terminal or notebook when you just want a quick look at a subset of data. ~ [Polars User Guide](https://pola-rs.github.io/polars-book/user-guide/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\n",
    "    pl.col('*').sample(n=5)\n",
    "])\n",
    "\n",
    "#   df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fcef14",
   "metadata": {},
   "source": [
    "We could still use the old way, as it is shorter. However, we can easily extend this expression to include more information if we want. It also helps us to solidify to work with expression in our workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1154321b",
   "metadata": {},
   "source": [
    "> The dataset has a lot to offer. With 11 variables and over 150k rows, there is a lot of data to analyse. We see a couple of variables that are interesting to look into, like price, country, points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e5e70c",
   "metadata": {},
   "source": [
    "To select a column, we use the following expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eee3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    pl.col(\"price\")\n",
    ").limit(10)\n",
    "\n",
    "# Earlier we used the following:\n",
    "#   df['price']\n",
    "# Select by index is considered an anti-pattern in Polars "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d095ef35",
   "metadata": {},
   "source": [
    "## Removing nulls\n",
    "\n",
    ">Before we continue we want to have a closer look at if there are any nulls in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28910f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "    pl.col(\"*\").null_count()\n",
    ")\n",
    "\n",
    "# Earlier we used the following: \n",
    "#   df.null_count()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f2832a",
   "metadata": {},
   "source": [
    "Similar to our original notebook we will fill the nulls with the mean for the following reason.\n",
    "\n",
    "> It seems that around a little less than 10% of the price variable has no value. We can either drop the rows with missing values or fill them. In this article, we will choose to use the mean as filling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff973c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.with_column(\n",
    "        pl.col(\"price\").fill_null(strategy=\"mean\").alias('price')\n",
    ")\n",
    "\n",
    "print(df.select('price').null_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57a423",
   "metadata": {},
   "source": [
    "## Some analyses\n",
    "\n",
    "> The next step is to dive in a little deeper and have a closer look at the dataset with some more complex functions.\n",
    "> The goal that we want to achieve in the following part is to have a closer look at the countries and how they compare in terms of price and points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b1df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\n",
    "        pl.median(\"price\").alias(\"median price\"),\n",
    "        pl.min(\"price\").alias(\"min price\"),\n",
    "        pl.max(\"price\").alias(\"max price\"),\n",
    "        pl.mean(\"price\").alias(\"mean price\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Analyses of wine prices\n",
    "# print(f'Median price: {data[\"price\"].median()}')\n",
    "# print(f'Average price: {data[\"price\"].mean()}')\n",
    "# print(f'Maximum price: {data[\"price\"].max()}')\n",
    "# print(f'Minimum price: {data[\"price\"].min()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b7221",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\n",
    "        pl.median(\"points\").alias(\"median points\"),\n",
    "        pl.min(\"points\").alias(\"min points\"),\n",
    "        pl.max(\"points\").alias(\"max points\"),\n",
    "        pl.mean(\"points\").alias(\"mean points\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Analyses of wine points\n",
    "# print(f'Median points: {data[\"points\"].median()}')\n",
    "# print(f'Average points: {data[\"points\"].mean()}')\n",
    "# print(f'Maximum points: {data[\"points\"].max()}')\n",
    "# print(f'Minimum points: {data[\"points\"].min()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de83495",
   "metadata": {},
   "source": [
    "Or we can just use the `describe()` function for quick statistics about our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d31ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d533ce",
   "metadata": {},
   "source": [
    "> The minimum number of points shows that there is no such thing as bad wine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c9a3e",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211f8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select([\n",
    "    pl.col(\"country\").unique()\n",
    "])\n",
    "\n",
    "# data['country'].unique().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a703506",
   "metadata": {},
   "source": [
    "From our earlier notebook\n",
    "\n",
    "> There are two strange values in our dataset: an undefined country (\"\") and a country called 'US-France'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eacca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(\n",
    "    (pl.col('country') == 'US-France') |\n",
    "    (pl.col('country').is_null())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aff472",
   "metadata": {},
   "source": [
    "> There are only 6 of them, so it is safe to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ")\n",
    "\n",
    "# data = data[(data['country'] != '') & (data['country'] != 'US-France')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a81d5d",
   "metadata": {},
   "source": [
    "> Time to look into the countries that produces the best wine according to the points and has the highest price for a bottle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb8c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('country').agg(\n",
    "    pl.col('points').mean().alias('points_mean')\n",
    ").sort(by='points_mean', reverse=True)\n",
    "\n",
    "# We group by country, select the `points` variable \n",
    "# and call the mean to see the average number of points. \n",
    "# After that we sort the list by 'average points'. \n",
    "\n",
    "# data.groupby('country')\n",
    "#       .select('points').mean()\n",
    "#       .sort(by_column='points_mean', reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b1181",
   "metadata": {},
   "source": [
    "> England is leading the list for the best wines. Wonder how they think about that on the other side of the Canal in France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26f3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('country').agg(\n",
    "    pl.col('price').max().alias('price_max')\n",
    ").sort(by='price_max', reverse=True)\n",
    "\n",
    "# data.groupby('country')\n",
    "#   .select('price').max()\n",
    "#   .sort(by_column='price_max', reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465d5a78",
   "metadata": {},
   "source": [
    "### Plotting with Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a17ce",
   "metadata": {},
   "source": [
    "> To get a better insight into the differences it always helps to have some nice plots. Where Pandas has a plotting functionality build in, we have to rely on our Matplotlib skills for Polars. We focus on the top 15 countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_15_countries = df.groupby('country').agg(\n",
    "    pl.col('points').mean().alias('points_mean')\n",
    ").sort(by='points_mean', reverse=True).limit(15)\n",
    "\n",
    "top_15_countries\n",
    "\n",
    "# Get a list of the top 15 countries by taking the first 15 rows \n",
    "# of the groupby that we did earlier\n",
    "# \n",
    "# top_15_countries = data.groupby('country')\n",
    "#       .select('points').mean()\n",
    "#       .sort(by_column='points_mean', reverse=True)[:15][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badeebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top15 = top_15_countries.join(df, on='country', how='left')\n",
    "\n",
    "df_top15\n",
    "\n",
    "# df_top15 = pl.DataFrame({'country': top_15_countries}).join(data, on='country', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18463db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "for i, country_df in enumerate(df_top15.partition_by(groups=\"country\")):\n",
    "    country_name = country_df.select(\"country\")[0, 0]\n",
    "    ax.boxplot(country_df.select('points'), labels=[country_name], positions=[i])\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Countries')\n",
    "plt.ylabel('Average points')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# for i, x in enumerate(df_top15['country'].unique()):\n",
    "#     ax.boxplot(df_top15[df_top15['country'] == x]['points'], labels=[str(x)], positions=[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d50469",
   "metadata": {},
   "source": [
    "## Time to go Lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec70a1",
   "metadata": {},
   "source": [
    "> The lazy API offers a way to optimise your queries, similar to Spark. The major benefit over spark is that we don't have to set up our environment and can therefore continue working from our notebook.\n",
    "\n",
    "> More information can be found in the [Polars-book](https://ritchie46.github.io/polars-book/lazy_polars/intro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cdccfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df = pl.scan_csv('winemag-data_first150k.csv')\n",
    "\n",
    "lazy_df\n",
    "\n",
    "# lazy_df = pl.scan_csv('winemag-data_first150k', ignore_errors=True)\n",
    "\n",
    "# if you install Graphviz you will see the Query Plan "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc1bcf",
   "metadata": {},
   "source": [
    "> Printing the type returns 'polars.lazy.LazyFrame' indicating the data is available to us. On to the Groupby `country` and find the average `points` to compare with the eager API that we used earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60ebe4",
   "metadata": {},
   "source": [
    "> Similar to the filters that we did with the eager API we are going to filter the unknown and 'US-France' values in the `country` variable first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f52ba8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ")\n",
    "\n",
    "# we can see that the query is almost the same\n",
    "# however this query only returns a query plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb8550",
   "metadata": {},
   "source": [
    "> As we can see nothing happens right away. From the documentation: '_This is due to the lazyness, nothing will happen until specifically requested. This allows Polars to see the whole context of a query and optimize just in time for execution._'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab84fae",
   "metadata": {},
   "source": [
    "> To actually see the results we can do two things: `collect()` and `fetch()`. The difference is that `fetch` takes the first 500 rows and then runs the query, whereas `collect` runs the query over all the results. Below we can see the differences for our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f59405",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ").fetch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.filter(\n",
    "    (pl.col('country').is_not_null()) &\n",
    "    (pl.col('country') != 'US-France')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb88e8d1",
   "metadata": {},
   "source": [
    "We can see from the shapes that `fetch` catches 500 rows and `collect` retrieves all the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f0f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.groupby('country').agg(\n",
    "    pl.col('points').mean().alias('points_mean')\n",
    ").sort(by='points_mean', reverse=True).collect()\n",
    "\n",
    "#  lazy_df = (\n",
    "#     lazy_df\n",
    "#     .groupby('country')\n",
    "#     .agg([pl.mean('points').alias('avg_points')])\n",
    "#     .sort(\"avg_points\", reverse=True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da4d959",
   "metadata": {},
   "source": [
    "### Out of Core [NEW]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2861bd96",
   "metadata": {},
   "source": [
    "What if you dataset doesn't fit in memory? This example is rather small, but in this day and age it is not unlikely that you are working on datsets that don't fit in memory any more. Polars offers a very easy way to work with that. \n",
    "\n",
    "Pretend that our dataset is not ~50MB, but 50GB. What can we do to, for example, Groupby country and do some calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da169279",
   "metadata": {},
   "source": [
    "Not much changes, except that in `collect()` we add: `streaming=True` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.groupby('country').agg(\n",
    "    pl.col('points').mean().alias('points_mean')\n",
    ").sort(by='points_mean', reverse=True).collect(streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e49021",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129ea48e",
   "metadata": {},
   "source": [
    "> We have got the output that we are looking for. Polars offers several ways to output our analyses, even to other formats useful for further analyses (e.g. pandas dataframe (`to_pandas()`) or numpy arrays (`to_numpy()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4387b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_df.collect().write_csv('results.csv')\n",
    "\n",
    "#  lazy_df.collect().to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f008df",
   "metadata": {},
   "source": [
    "## Final words from 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0239a835",
   "metadata": {},
   "source": [
    "> Polars is a new package that is gaining a lot of attention. At the time of writing this article, it has gathered more than 1300 stars on Github, which is impressive looking at the fact that is around for less than a year. It offers almost all the functions that we need to manipulate our dataframe. Next to that, it offers a lazy API that helps us optimising our queries before we execute them. Although we didn't touch it is in this article, the benchmark of H20 shows that it is super efficient and fast. Especially with larger datasets it becomes worthwhile to look into the benefits that the lazy API has to offer.\n",
    "\n",
    "This is what I wrote two years ago. 1300 stars. In 2022, while I am writing this, the project has collected 10.4k stars on Github and it is number 4 trending project on Github.\n",
    "\n",
    "There are a lot of improvements happening under the hood and in the APIs. \n",
    "\n",
    "For more information, check the [Polars' Github page](https://github.com/pola-rs/polars). Here you can find links to benchmark's, the Polars book or the Polars Discord. \n",
    "\n",
    "I highly recommend joining the Discord. There is a lot of activity and many people are happy to help you with your specific questions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7cfc7b723e3ad961d8ad0c4a874ed99db38e2daae87ff284caf4b74a64b262de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
